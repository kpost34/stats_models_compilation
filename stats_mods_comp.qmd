---
title: "Compilation of Statistical and Modelling Techniques"
author: "Keith Post"
format: html
editor: visual
---

# Introduction

This document will provide a concise but sufficient coverage of a variety of statistical and modelling techniques, which will include assumptions, coding basics, interpretation, and links to references.

```{r}
# Load packages & source functions
pacman::p_load(here, tidyverse, rstatix, DT, janitor, datasets, ggfortify, lmtest, broom, GGally)

source(here("objs_and_fns", "00_helper-fns.R"))
```

# Parametric Statistics

## t-test (unpaired, 2-sample)

### Generate data

Data will be generated through sampling of normal distributions to produce the two samples. These samples will be combined into a tibble and pivoted to long format. Here is a sample of the data.

```{r}
# Generate simulated data
## Simulate data & combine into tibble
set.seed(33)
x <- rnorm(30, 0, 1)
y <- rnorm(30, 0.5, 1)

df_samp <- tibble(x, y) %>%
  pivot_longer(cols=everything(), names_to="var", values_to="value")


## Show in table (dat sample)
df_samp %>%
  mutate(value=signif(value, 3)) %>%
  head(10) %>%
  DT::datatable(options=list(dom="t"), rownames=FALSE, caption="Data sample")
```

### Assumptions (<https://www.statology.org/t-test-assumptions/>)

There are four assumptions of a t-test:

1.  Independent observations (per sample)
2.  Approximately normally distributed data (per sample)
3.  Equal variance
4.  Random sampling

The two assumptions listed above that are testable statistically are normal distribution of data and equal variance.

```{r}
# Test assumptions (that are testable)
## Normal distribution
#plot histograms
df_samp %>%
  ggplot(aes(x=value, fill=var)) +
  geom_histogram(aes(y=after_stat(density)), alpha=0.8, bins=10, color="black") +
  geom_density(alpha=0, lwd=2) +
  scale_fill_manual(values=c("x"="steelblue", "y"="darkgreen")) +
  facet_wrap(~var) +
  labs(fill="") +
  theme_bw(base_size=14) +
  theme(legend.position="bottom")
#appear normal


#Shappiro tests
df_samp %>%
  group_by(var) %>%
  shapiro_test(value)
#neither is signifcant--which suggest normal distribution


## Equal variance
#Levene's test
df_samp %>%
  levene_test(value ~ var) #0.664; NS, thus similar variance

```

The testing of these assumptions indicate that the samples are normally distributed and have equal variance. This is unsurprising given that they are derived from normal distributions with the same standard deviations.

### Run t-test

The t-test will be run on the x and y samples. This will be a two-sided t-test (i.e., default alternative argument set to "two.sided") with equal variance (i.e., the var.equal argument set to TRUE ) and a default alpha level of 0.05 (conf.level = 0.95 (default)).

```{r}
# Run t-test
t_test(df_samp, formula=value ~ var, var.equal=TRUE) %>%
  mutate(statistic=signif(statistic, 3)) %>%
  select(-`.y.`) %>%
  datatable(options=list(dom="t"))
```

The result indicates that the test is *not* significant because *p* \> 0.05. This means that the populational averages of groups x and y are *not* statistically different.

## ANOVA

### One-way ANOVA

Let's generate three samples.

```{r}
# Generate and preview data
## Simulate data
set.seed(21)
x <- rnorm(20, 6, 2)
y <- rt(20, 5, 4)
z <- rt(20, 5, 5)

# y <- rt(30, 5, 2.7)
# z <- rt(30, 10, 2)

df_samp_anova <- tibble(x, y, z) %>%
  pivot_longer(everything(), names_to="group", values_to="value")


## Show in table (dat sample)
df_samp_anova %>%
  mutate(value=signif(value, 3)) %>%
  head(10) %>%
  DT::datatable(options=list(dom="t"), rownames=FALSE, caption="Data sample")

```

#### Assumptions (INSERT REF)

There are three assumptions of an ANOVA:

1.  Independent observations
2.  Residuals are normally distributed
3.  Homogeneity of variances

The second two assumptions can be tested in R:

```{r}
# Test ANOVA assumptions
## Normal distribution of residuals
#grab residuals
df_resid_anova <- aov(value ~ group, data=df_samp_anova) %>%
  resid() %>%
  tibble(resid=.)

#plot histogram of them
df_resid_anova %>%
  ggplot(aes(x=resid)) +
  geom_histogram(aes(y=after_stat(density)), alpha=0.8, bins=10, 
                 color="black", fill="steelblue") +
  geom_density(alpha=0, lwd=2) +
  theme_bw(base_size=14) +
  theme(legend.position="bottom")

#shapiro test
df_resid_anova %>%
  shapiro_test(resid)
#significant (p < 0.007)


## Homogeneity of variances
df_samp_anova %>%
  make_anova_boxplot()
#spread of data among groups appears similar


df_samp_anova %>%
  levene_test(value ~ group)
#NS; p > 0.5
```

The results show that the residuals are *not* normally distributed as evidenced by the right-skewed histogram of residuals and significant Shapiro test result. Conversely, the variances among groups is similar. The boxplots show similar spreads in data among x, y, and z and the Levene's test was not significant. In this case, a data transformation such as the log transformation is commonly performed to achieve normality of residuals.

```{r}
# Data Transformation
df_samp_anova_transform <- df_samp_anova %>%
  #log is a common transformations
  mutate(log_value=log(value)) 

df_samp_anova_transform_long <- df_samp_anova_transform %>%
  pivot_longer(cols=ends_with("value"),
               names_to="value_type",
               values_to="value") %>%
  mutate(value_type=ifelse(value_type=="value",
                           "raw",
                           "log"))

df_resid_anova_transform <- df_resid_anova %>%
  rename(raw="resid") %>%
  bind_cols(
    log = aov(log_value ~ group, data=df_samp_anova_transform) %>% resid()) %>%
  pivot_longer(cols=everything(), names_to="transform", values_to="resid", 
               names_prefix="resid_")

  

# Re-test assumptions
#plot histogram of them
df_resid_anova_transform %>%
  mutate(transform=factor(transform, levels=c("raw", "log", "sqrt"))) %>%
  ggplot(aes(x=resid)) +
  geom_histogram(aes(y=after_stat(density), fill=transform), alpha=0.8, bins=10, 
                 color="black") +
  geom_density(alpha=0, lwd=2) +
  scale_fill_manual(values=c("raw"="gray60", "log"="steelblue")) +
  facet_wrap(~transform, nrow=3, scales="free") +
  labs(fill="") +
  theme_bw(base_size=14) +
  theme(legend.position="none")
#log transformation creates a more symmetrical distribution

#shapiro test
df_resid_anova_transform %>%
  group_by(transform) %>%
  shapiro_test(resid)
#log-transform = NS


## Homogeneity of variances
df_samp_anova_transform_long %>%
  group_by(value_type) %>%
  levene_test(value ~ group)
#log-transform: NS
```

The data were log-transformed and their residuals plotted as histograms followed and tested for normality. The plot of residuals of the log-transformed data indicate greater symmetry, which is supported by a non-significant result from the Shapiro test results. A Levene's test was also non-significant, indicating homogeneity among variances.

Given these results, boxplots of each group were constructed using both raw and log-transformed data.

```{r}
# Boxplots of data among groups
df_samp_anova_transform_long <- df_samp_anova_transform %>%
  pivot_longer(cols=ends_with("value"), 
               names_to="transform_type", 
               values_to="value")

df_samp_anova_transform_long %>%
  mutate(transform_type=ifelse(transform_type=="value",
                               "raw",
                               str_replace(transform_type, 
                                           "_value$", 
                                           "-transformed")),
         transform_type=factor(transform_type, levels=c("raw", "log-transformed"))) %>%
  make_anova_boxplot(facet=TRUE)
```

Both sets of boxplots show that groups x and z have larger medians (and likely means) than group z. A one-way ANOVA was conducted on the log-transformed data to test for significance differences.

```{r}
# Run ANOVA 
anova_mod <- aov(log_value ~ group, data=df_samp_anova_transform)

summary(anova_mod)[[1]] %>% 
  clean_names() %>%
  mutate(across(.cols=!df, ~signif(.x, 3)),
         p_value_sig=ifelse(pr_f <= 0.05,
                            "*",
                            "ns")) %>%
  rename(p_value="pr_f") %>%
  DT::datatable(options=list(dom="t"))
#p < 0.017

# Run post-hoc Tukey's HSD test
tukey_mod <- df_samp_anova_transform %>%
  tukey_hsd(log_value ~ group)

tukey_mod %>%
  clean_names() %>%
  select(-c(term, null_value)) %>%
  mutate(across(.cols=c(estimate, conf_low, conf_high, p_adj),
                ~signif(.x, 3))) %>%
  DT::datatable(options=list(dom="t"))
#significant difference between groups y and z 
#trend for x-y
#NS for x-z

# Create forest plot
tukey_mod %>%
  select(-null.value) %>%
  mutate(comp=paste(group1,group2,sep="-"),
         comp=fct_relevel(comp,c("x-y","x-z","y-z"))) %>%
  ggplot() +
  geom_linerange(aes(x=comp,ymin=conf.low,ymax=conf.high)) +
  geom_hline(yintercept=0,linetype=2,color="blue") +
  geom_point(aes(x=comp,y=estimate),size=4) +
  geom_text(aes(x=comp,y=estimate,label=paste0("p=",p.adj)),
            size=6, nudge_x=.1, nudge_y=.05) +
  coord_flip() +
  labs(x="Mean comparison",
       y="Mean difference") +
  theme_bw(base_size=17) 


```

The results of the one-way ANOVA indicate a significant differ among the three groups as evidenced by *p* \< 0.02. A *post hoc* Tukey's HSD test was conducted among all pairwise comparisons to identify the source of the difference. This showed that group y was significantly smaller than group z (*p* \< 0.02). The forest plot shows that the confidence interval surrounding the mean differences for all pairwise comparisons. Only x-z does not contain 0; thus, this difference is significant at alpha = 0.05.

# Linear Regression

## Simple Linear Regression

#### Introduction

Real data instead of simulated data will be used for this statistical model. The data will come from the *cars* dataset in the {datasets} package. It is a simple data frame with two variables--speed (numeric speed of the car in miles per hour) and dist (stopping distance in feet)\--and there are 50 rows of data. This sets up nicely for a simple linear regression, which contains a predictor or independent variable (x) and a dependent variable (y) and follows the form *y* = *mx* + *b*, where *m* is the slope of the line and *b* is the y-intercept. A simple linear regression has four assumptions.

#### Assumptions (REFERENCE = <https://www.jmp.com/en_us/statistics-knowledge-portal/what-is-regression/simple-linear-regression-assumptions.html>

1.  Linear relationship between x and y
2.  Normally distributed errors
3.  Constant variance in y
4.  Independent observations

Let's assess these as best as we can. We will assume that the observations are independent, but the others can be evaluated through statistical methods.

```{r}
# Test Assumptions with Untransformed Data
## Linear relationship
cars %>%
  ggplot(aes(x=speed, y=dist)) +
  geom_point() +
  theme_bw()
#this scatter plot indicates a positive, linear relationship between speed and dist

## Normally distributed errors
linear_mod <- lm(dist ~ speed, cars) 

df_lm_resid <- resid(linear_mod) %>%
  tibble(resid=.)

df_lm_resid %>%
  ggplot() +
  geom_histogram(aes(x=resid), bins=40, color="black", fill="steelblue") +
  theme_bw()
#appears non-normal

shapiro_test(df_lm_resid, resid) #significant

```

A scatter plot of the data showed that the first assumption of a linear relationship was met. A linear model of the data was developed and the residuals were extracted and plotted in a histogram. This plot did not indicate normality in the data. A Shapiro test was also conducted, which was significant at an alpha = 0.05 (*p* \< 0.05).

Given that the assumptions were not met, dist values were log transformed and the assumptions re-tested.

```{r}
# Log-transformed Data
## Transformation
cars %>%
  mutate(log_dist=log(dist)) -> df_log_cars

## Linear relationship
df_log_cars %>%
  ggplot() +
  geom_point(aes(x=speed, y=log_dist)) +
  theme_bw()
#clearly linear relationship

## Construct model
linear_mod_log <- lm(log_dist ~ speed, df_log_cars)


## Grab residuals
df_lm_resid_log <- resid(linear_mod_log) %>%
  tibble(resid=.)

## Test normality of residuals
df_lm_resid_log %>%
  ggplot() +
  geom_histogram(aes(x=resid), bins=40, color="black", fill="steelblue") +
  theme_bw()
#distribution looks more symmetrical following log transformation

shapiro_test(df_lm_resid_log, resid) #NS


## Constant variance in y
autoplot(linear_mod_log)[c(1, 3)] 
#not indicative of constant variance; it decreases with increasing x

bptest(linear_mod_log) #significant

```

The dependent variable, dist, was log-transformed and the assumptions re-tested. The data appeared to have a linear relationship and this time the residuals were normally distributed as evidenced by the histogram and Shapiro test. However, the constant variance assumption was not met. Plots of the residuals versus fitted values and square roots of the standardized residuals versus fitted values indicated decreasing variance as x (speed) increases. This was supported by a significant Breusch-Pagan test (*p* \< 0.004).

Given these results, dist was square-root transformed and the assumptions were re-tested.

```{r}
# Sqrt-transform
cars %>%
  mutate(sqrt_dist=sqrt(dist)) -> df_sqrt_cars

## Linear relationship
df_sqrt_cars %>%
  ggplot() +
  geom_point(aes(x=speed, y=sqrt_dist)) +
  theme_bw()
#clearly linear relationship

## Fit model and grab residuals
linear_mod_sqrt <- lm(sqrt_dist ~ speed, df_sqrt_cars)

df_lm_resid_sqrt <- resid(linear_mod_sqrt) %>%
  tibble(resid=.)

## Normality of residuals
df_lm_resid_sqrt %>%
  ggplot() +
  geom_histogram(aes(x=resid), bins=30, color="black", fill="steelblue") +
  theme_bw()
#distribution looks more symmetrical following sqrt transformation

shapiro_test(df_lm_resid_sqrt, resid) #NS


## Constant variance in y
### Grapically
autoplot(linear_mod_sqrt)[c(1, 3)]
#although neither plot has a straight, horizontal line, there is no consistent trend. Let's test it statistically.

### Statistically (via test)
#use a Breusch-Pagan test
bptest(linear_mod_sqrt) #NS
#indicates that Ho of no correlation between fit and residuals is not rejected

```

The linear relationship assumption was met per the scatter plot of the square-root transformed dist and speed. The second assumption of normality of residuals was also met. They appeared to have a normal distribution, which was supported by a Shapiro test. Finally, the square-root transformed dist had constant variance in y per the visualizations and Breusch-Pagan test. Thus, the analysis will be conducted on square-root transformed data.

### Run the Analysis

```{r}
# Run the analysis
## Summary info
summary(linear_mod_sqrt)
#p < 1.78 x 10^-14 & R^2 = 0.7094
#formula = sqrt_dist = 0.32241*speed + 1.27705

## Model parameters & statistics
tidy(linear_mod_sqrt) %>%
  bind_cols(
    glance(linear_mod_sqrt) %>%
      select(r.squared)
  ) %>%
  mutate(across(!term, ~signif(.x, 3))) %>%
  DT::datatable()
#significant intercept and slope, and model explains ~ 71% of variance in dist


# Plot results
## Grab predicted values and CI information
### Build sequence of speeds
vec_speed_range <- range(df_sqrt_cars$speed) 
df_speed_seq <- seq(vec_speed_range[1], vec_speed_range[2], length.out=200) %>%
  tibble(speed=.)


### Generate predicted values and CI & PI bands
df_pred_sqrt_dist <- c("confidence", "predict") %>%
  #generate pred values with bands (CI or PI)
  purrr::map(function(x) {
    predict(linear_mod_sqrt, df_speed_seq, 
            interval=x) %>%
      as_tibble() %>%
      {if (x=="confidence") set_names(., c("fit", "ci_lwr", "ci_upr"))
        else set_names(., c("fit2", "pi_lwr", "pi_upr"))}
  }) %>%
  #bind results together with x (speed) data fed into model
  bind_cols(df_speed_seq) %>% 
  select(-fit2) %>%
  relocate(speed) %>%
  rename_with(.cols=!speed, .fn=~paste0("dist_sqrt__", .x)) %>%
  #back-convert fitted and bands of intervals to raw dist
  mutate(across(!speed, ~.x^2, 
                .names="{.col}2")) %>% 
  rename_with(.cols=ends_with("2"), .fn=~str_remove_all(.x, "_sqrt|2")) %>%
  #pivot to multiple values
  pivot_longer(cols=!speed,
               names_to=c("dist_type", ".value"),
               names_pattern="(.+)__(.+)") %>%
  mutate(dist_type=str_replace(dist_type, "dist_sqrt", "sqrt_dist"))

  
## Plot data and predicted values
### With regression line using raw and transformed data
df_sqrt_cars %>%
  pivot_longer(cols=!speed, names_to="dist_type", values_to="y") %>%
  mutate(dist_type=factor(dist_type, levels=c("sqrt_dist", "dist"))) %>%
  ggplot(aes(x=speed, y=y)) +
  geom_point(aes(color=dist_type)) +
  geom_line(data=df_pred_sqrt_dist, 
            aes(x=speed, y=fit)) +
  geom_ribbon(data=df_pred_sqrt_dist,
              aes(x=speed, y=fit, ymin=ci_lwr, ymax=ci_upr),
              fill="gray50", alpha=.3) +
  geom_ribbon(data=df_pred_sqrt_dist,
              aes(x=speed, y=fit, ymin=pi_lwr, ymax=pi_upr),
              fill="gray20", alpha=.3) +
  facet_wrap(~dist_type, scales="free") +
  labs(y="") +
  theme_bw(base_size=15)  +
  theme(legend.position="none")
#faceted figure with dist (left) and sqrt_dist (right)
#includes data as points, regression line, and both CI (darker, narrower), and
  #PI (lighter, wider) bands
#expect non-linear trend in left plot because this shows back-transformed predictions
  #and not a direct fit of the data
#both CI bands are narrow and show consistent variance throughout the range with only
  #slight widening of CI bands at extremes


```

The results indicate that both the slope (*p* \< 2 x 10\^14) and intercept (*p* \< 0.05) were significant with a strong relationship (R \^2 = 0.709). The linear model for this relationship as is follows:

*sqrt_dist* = 0.322 \* speed + 1.28

Unsurprisingly, back-transformation of the data (by squaring dist) yields a positive, non-linear regression line.

## Multiple Linear Regression

#### Introduction

Real data again will be used to illustrate multiple linear regression. The data will come from the *mtcars* dataset in the {datasets} package. The *mtcars* dataset contains fuel efficiency and 10 variables related to the engine, transmission, design, and performance for 32 automobile models (i.e., 32 rows by 11 columns) produced from 1973-1974 that may influence fuel consumption.

```{r}
mtcars %>%
  DT::datatable(rownames=FALSE, options=list(dom="tlip"))
```

The dataset encodes all variables as *double*, but it is apparent that many variables are integers (including binary variables), and thus can be re-classified.

```{r}
mtcars %>%
  distinct(cyl, vs, am, gear, carb) %>%
  DT::datatable()
```

In this section, multiple linear regression will be used with mtcars to develop a model between the dependent variable, *mpg*, and multiple predictor variables. To simplify the analysis, a subset of predictors will be used. Here's a list of them with their associated descriptions:

-   *disp*: displacement (cu. in.)

-   *hp*: gross horsepower

-   *wt*: weight (1000 lbs)

-   *vs*: engine (0 = V-shaped, 1 = straight)

-   *mpg*: miles per (US) gallon \[**dependent variable**\]

Let's look at the assumptions.

#### Assumptions (REFERENCE =

Like simple linear regression, multiple linear regression has the same four assumptions *and* lack of multicollinearity. Thus, they are as follows:

1.  Linear relationship between x and y
2.  Normally distributed errors
3.  Constant variance in y
4.  Independent observations
5.  No multicollinearity among predictors

Let's test assumptions 1-3 and 5 statistically. First, a linear relationship between each predictor and the dependent variable, *mpg*, will be assessed visually using scatter plots. Note, that *vs* does not need to be tested as it is a binary, dummy variable.

```{r}
# Test assumption 1: linear relationship
mtcars_mlr <- mtcars %>%
  select(disp, hp, wt, vs, mpg)

pairs <- mtcars_mlr %>% 
  select(-vs) %>%
  ggpairs() 

plots <- lapply(1:(pairs$ncol-1), function(j) getPlot(pairs, i=4, j=j))

ggmatrix(
  plots,
  nrow=1,
  ncol=pairs$ncol-1,
  xAxisLabels=pairs$xAxisLabels[1:3],
  yAxisLabels="mpg"
) 

``
```

These plots clearly indicate linear relationship between *mpg* and the three numerical predictors: *disp*, *hp*, and *wt*.

Next, assumption 2 will be assessed using

### Logistic Regression

### Generalized Linear Models

### Generalized Linear Mixed Models

### Splines

### AB Testing
